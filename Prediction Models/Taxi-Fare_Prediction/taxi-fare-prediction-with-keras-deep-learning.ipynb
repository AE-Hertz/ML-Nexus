{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "202e10c43907e5fdc7264a2c328aa0a53ed73a10"
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, BatchNormalization\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2f2e006756ff46df570d539cc3976520d530cb6"
   },
   "source": [
    "## Data clean\n",
    "### Here i'm removing some outliers, and noisy data.\n",
    "* Lats and lons that do not belong to New York.\n",
    "* Negative fare.\n",
    "* Fare greater than 250 (this seems to be noisy data).\n",
    "* Rides that begin and end in the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "716e84668e42e06342b58423ea1f64a319d05f62"
   },
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    # Delimiter lats and lons to NY only\n",
    "    df = df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]\n",
    "    df = df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]\n",
    "    df = df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]\n",
    "    df = df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]\n",
    "    # Remove possible outliers\n",
    "    df = df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]\n",
    "    # Remove inconsistent values\n",
    "    df = df[(df['dropoff_longitude'] != df['pickup_longitude'])]\n",
    "    df = df[(df['dropoff_latitude'] != df['pickup_latitude'])]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12bb5164c954464788a0b26afb62d13f8a5f868c"
   },
   "source": [
    "## Feature engineering\n",
    "*  Now i'll do some feature engineering and process the data, i'm basically creating 3 kinds of features.\n",
    "    *  **Time features**\n",
    "        * Year, Month, Day, Hour, Weekday\n",
    "        * Night (between 16h and 20h, from monday to friday)\n",
    "        * Late night (between 20h and and 6h)\n",
    "    * **Coordinate features**\n",
    "        * Latitude difference (difference from pickup and dropout latitudes)\n",
    "        * Longitude difference (difference from pickup and dropout longitudes)\n",
    "    * **Distances features**\n",
    "        * Euclidean (Euclidean distance from pickup and dropout)\n",
    "        * Manhattan (Manhattan distance from pickup and dropout)\n",
    "        * Manhattan distances from pickup location and downtown, JFK, EWR and LGR airports (see if the ride started at one of these locations).\n",
    "        * Manhattan distances from dropout location and downtown, JFK, EWR and LGR airports (see if the ride ended at one of these locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def late_night (row):\n",
    "    if (row['hour'] <= 6) or (row['hour'] >= 20):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def night (row):\n",
    "    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n",
    "\n",
    "\n",
    "def add_time_features(df):\n",
    "    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n",
    "    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n",
    "    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n",
    "    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n",
    "    df['night'] = df.apply (lambda x: night(x), axis=1)\n",
    "    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)\n",
    "    # Drop 'pickup_datetime' as we won't need it anymore\n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_coordinate_features(df):\n",
    "    lat1 = df['pickup_latitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "    \n",
    "    # Add new features\n",
    "    df['latdiff'] = (lat1 - lat2)\n",
    "    df['londiff'] = (lon1 - lon2)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_distances_features(df):\n",
    "    # Add distances from airpot and downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    \n",
    "    lat1 = df['pickup_latitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "    \n",
    "    df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n",
    "    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n",
    "    \n",
    "    df['downtown_pickup_distance'] = manhattan(ny[1], ny[0], lat1, lon1)\n",
    "    df['downtown_dropoff_distance'] = manhattan(ny[1], ny[0], lat2, lon2)\n",
    "    df['jfk_pickup_distance'] = manhattan(jfk[1], jfk[0], lat1, lon1)\n",
    "    df['jfk_dropoff_distance'] = manhattan(jfk[1], jfk[0], lat2, lon2)\n",
    "    df['ewr_pickup_distance'] = manhattan(ewr[1], ewr[0], lat1, lon1)\n",
    "    df['ewr_dropoff_distance'] = manhattan(ewr[1], ewr[0], lat2, lon2)\n",
    "    df['lgr_pickup_distance'] = manhattan(lgr[1], lgr[0], lat1, lon1)\n",
    "    df['lgr_dropoff_distance'] = manhattan(lgr[1], lgr[0], lat2, lon2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a098dcf43d5c5f8bbe1dbd6e89f797160f8173d5"
   },
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "646281e5cb269122cfe005191271d57bdcf42a96"
   },
   "outputs": [],
   "source": [
    "def output_submission(raw_test, prediction, id_column, prediction_column, file_name):\n",
    "    df = pd.DataFrame(prediction, columns=[prediction_column])\n",
    "    df[id_column] = raw_test[id_column]\n",
    "    df[[id_column, prediction_column]].to_csv((file_name), index=False)\n",
    "    print('Output complete')\n",
    "    \n",
    "    \n",
    "def plot_loss_accuracy(history):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f26c4a260b408ffbd1e881d205eb7145a1e37ce"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a67bbf9dc5b0caab8d964837c072b229b6224a5a"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../input/train.csv'\n",
    "TEST_PATH = '../input/test.csv'\n",
    "SUBMISSION_NAME = 'submission.csv'\n",
    "\n",
    "# Model parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "DATASET_SIZE = 6000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6e4c15bf38a9f2606d0fb2f65f152eaee7912df"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99506a559b162562260d3e347558249c0ed85b7f"
   },
   "outputs": [],
   "source": [
    "# Load values in a more compact form\n",
    "datatypes = {'key': 'str', \n",
    "              'fare_amount': 'float32',\n",
    "              'pickup_datetime': 'str', \n",
    "              'pickup_longitude': 'float32',\n",
    "              'pickup_latitude': 'float32',\n",
    "              'dropoff_longitude': 'float32',\n",
    "              'dropoff_latitude': 'float32',\n",
    "              'passenger_count': 'uint8'}\n",
    "\n",
    "# Only a fraction of the whole data\n",
    "train = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE, dtype=datatypes, usecols=[1,2,3,4,5,6])\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f7c8d4f7d38887e62deb95d0c5be74975ab22f0"
   },
   "source": [
    "#### Clean and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "061247c80aa6f3a443a59bd7736de33985cd1a7b"
   },
   "outputs": [],
   "source": [
    "train = clean(train)\n",
    "\n",
    "train = add_time_features(train)\n",
    "test = add_time_features(test)\n",
    "\n",
    "add_coordinate_features(train)\n",
    "add_coordinate_features(test)\n",
    "\n",
    "train = add_distances_features(train)\n",
    "test = add_distances_features(test)\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b06b6025ec3822623a120c06170e74161f0f7262"
   },
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "dropped_columns = ['pickup_longitude', 'pickup_latitude', \n",
    "                   'dropoff_longitude', 'dropoff_latitude']\n",
    "train_clean = train.drop(dropped_columns, axis=1)\n",
    "test_clean = test.drop(dropped_columns + ['key', 'passenger_count'], axis=1)\n",
    "\n",
    "# peek data\n",
    "train_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "736872d7fb45a0b3e7532db4da69da29ac4761fd"
   },
   "source": [
    "#### Split data in train and validation (90% ~ 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea58d88d113808dc224e176db2bc37f6c93e08db"
   },
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)\n",
    "\n",
    "# Get labels\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9500aaf4067e9c953d045421f767e8a536a0efad"
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df)\n",
    "validation_df_scaled = scaler.transform(validation_df)\n",
    "test_scaled = scaler.transform(test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ccc5e6695f687e26444532b1c53849e894222ad"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "8dd2826039f92c6d46fde06c9c030ff76bb5933f"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfe2b169b769e5e887523c4b5b00b8320599ffb3"
   },
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b8ba37654ff52b8d7d615090ba5fa60c85f322cf"
   },
   "outputs": [],
   "source": [
    "print('Dataset size: %s' % DATASET_SIZE)\n",
    "print('Epochs: %s' % EPOCHS)\n",
    "print('Learning rate: %s' % LEARNING_RATE)\n",
    "print('Batch size: %s' % BATCH_SIZE)\n",
    "print('Input dimension: %s' % train_df_scaled.shape[1])\n",
    "print('Features used: %s' % train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25f481043932ddb2e57a0b7199bcb42d4a11c44e"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14ddb39832ed9661b987abaed3a3a0d4397a1fe2"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0d16fe6d7fcc97e819e2c1168c38565ef34267e"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "034ee7cc55876cb8fbb4d030b988074b68ff5c97"
   },
   "source": [
    "### Plot metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bba904fee24ae03e106a55e77ba6e43943c35b9a"
   },
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb1f3931f926c01a966f17940fc134f1800ca39c"
   },
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "prediction = model.predict(test_scaled, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "faca9351377875f0dbfc36306c5470d3e95e313c"
   },
   "outputs": [],
   "source": [
    "# output prediction\n",
    "output_submission(test, prediction, 'key', 'fare_amount', SUBMISSION_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
