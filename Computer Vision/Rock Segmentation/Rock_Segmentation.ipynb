{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbJYD2n2P2E1",
    "outputId": "96cc8d34-ea51-46ac-a01e-f232e3f5b997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/romainpessia/artificial-lunar-rocky-landscape-dataset\n",
      "License(s): CC-BY-NC-SA-4.0\n",
      "Downloading artificial-lunar-rocky-landscape-dataset.zip to /content\n",
      "100% 5.01G/5.02G [00:43<00:00, 153MB/s]\n",
      "100% 5.02G/5.02G [00:43<00:00, 124MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d romainpessia/artificial-lunar-rocky-landscape-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jVBcDxPQQqD3"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('/content/artificial-lunar-rocky-landscape-dataset.zip', 'r')\n",
    "zip_ref.extractall('/content')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I5-1avikN5xj"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose,AveragePooling2D, MaxPooling2D,UpSampling2D,LeakyReLU, concatenate, Dropout,BatchNormalization,Activation\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "InputPath = \"/content/images/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9kU35KHLEYim"
   },
   "outputs": [],
   "source": [
    "def Generator(X, y, batch_size=1):\n",
    "    while True:\n",
    "        img_batch_1 = []\n",
    "        img_batch_2 = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            i = np.random.randint(0, len(X))\n",
    "\n",
    "            img_1 = cv.imread(InputPath + 'render/' + X[i])\n",
    "            img_1 = cv.resize(img_1, (512, 512))\n",
    "            img_1 = img_1 / 255.0\n",
    "\n",
    "            img_2 = cv.imread(InputPath + 'ground/' + y[i])\n",
    "            img_2 = cv.resize(img_2, (512, 512))\n",
    "            img_2 = img_2 / 255.0\n",
    "\n",
    "            img_batch_1.append(img_1)\n",
    "            img_batch_2.append(img_2)\n",
    "\n",
    "        yield np.array(img_batch_1), np.array(img_batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PEyL-kwJFdgx"
   },
   "outputs": [],
   "source": [
    "SourceImg = sorted(os.listdir(InputPath+'render'))\n",
    "TargetImg = sorted(os.listdir(InputPath+'ground'))\n",
    "Gen = Generator(SourceImg,TargetImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoZX6WEAGANI",
    "outputId": "ec517958-93e6-42c9-8cdd-dcd8da7cdd0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 512, 512, 3), (1, 512, 512, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(Gen)[0].shape,next(Gen)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlvohs9NTc8Q",
    "outputId": "534b5c90-76f4-41c1-d126-87f0b6e6fe84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "input_layer False (None, 512, 512, 3)\n",
      "block1_conv1 False (None, 512, 512, 64)\n",
      "block1_conv2 False (None, 512, 512, 64)\n",
      "block1_pool False (None, 256, 256, 64)\n",
      "block2_conv1 False (None, 256, 256, 128)\n",
      "block2_conv2 False (None, 256, 256, 128)\n",
      "block2_pool False (None, 128, 128, 128)\n",
      "block3_conv1 False (None, 128, 128, 256)\n",
      "block3_conv2 False (None, 128, 128, 256)\n",
      "block3_conv3 False (None, 128, 128, 256)\n",
      "block3_pool False (None, 64, 64, 256)\n",
      "block4_conv1 False (None, 64, 64, 512)\n",
      "block4_conv2 False (None, 64, 64, 512)\n",
      "block4_conv3 False (None, 64, 64, 512)\n",
      "block4_pool False (None, 32, 32, 512)\n",
      "block5_conv1 True (None, 32, 32, 512)\n",
      "block5_conv2 True (None, 32, 32, 512)\n",
      "block5_conv3 True (None, 32, 32, 512)\n",
      "block5_pool True (None, 16, 16, 512)\n"
     ]
    }
   ],
   "source": [
    "VGG16 = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(512, 512,3))\n",
    "VGG16.trainable = False\n",
    "for layer in VGG16.layers:\n",
    "  if layer.name.startswith('block5'):\n",
    "    layer.trainable = True\n",
    "  print(layer.name,layer.trainable,layer.output.shape)\n",
    "last_layer = VGG16.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGEmVOfWUKQV"
   },
   "outputs": [],
   "source": [
    "model = Conv2DTranspose(256,(3,3),strides=(2, 2),padding='same')(last_layer)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "concat_1 = concatenate([model,VGG16.get_layer(\"block5_conv3\").output])\n",
    "\n",
    "model = Conv2D(512,(3,3),strides=(1, 1),padding='same')(concat_1)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2DTranspose(512,(3,3),strides=(2, 2),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "concat_2 = concatenate([model,VGG16.get_layer(\"block4_conv3\").output])\n",
    "\n",
    "model = Conv2D(512,(3,3),strides=(1, 1),padding='same')(concat_2)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2DTranspose(512,(3,3),strides=(2, 2),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "concat_3 = concatenate([model,VGG16.get_layer(\"block3_conv3\").output])\n",
    "\n",
    "model = Conv2D(256,(3,3),strides=(1, 1),padding='same')(concat_3)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2DTranspose(256,(3,3),strides=(2, 2),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "concat_4 = concatenate([model,VGG16.get_layer(\"block2_conv2\").output])\n",
    "\n",
    "model = Conv2D(128,(3,3),strides=(1, 1),padding='same')(concat_4)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2DTranspose(128,(3,3),strides=(2, 2),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "concat_5 = concatenate([model,VGG16.get_layer(\"block1_conv2\").output])\n",
    "\n",
    "model = Conv2D(64,(3,3),strides=(1, 1),padding='same')(concat_5)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2D(32,(3,3),strides=(1, 1),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Conv2D(3,(3,3),strides=(1, 1),padding='same')(model)\n",
    "model = LeakyReLU(0.1)(model)\n",
    "model = BatchNormalization()(model)\n",
    "\n",
    "model = Model(VGG16.input,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ptl5eW6Y8kt8"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvG90nOYIwCD"
   },
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint('UNET.keras', verbose=1,mode='auto', monitor='loss',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsRf8VJNJRt_",
    "outputId": "8efefa00-bd85-480c-b8c5-dc060f297f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m9766/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4929 - loss: 0.4110\n",
      "Epoch 1: loss improved from inf to 0.24264, saving model to UNET.keras\n",
      "\u001b[1m9766/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2482s\u001b[0m 250ms/step - accuracy: 0.4929 - loss: 0.4110\n",
      "Epoch 2/30\n",
      "\u001b[1m9766/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4807 - loss: 0.1251\n",
      "Epoch 2: loss improved from 0.24264 to 0.12159, saving model to UNET.keras\n",
      "\u001b[1m9766/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2433s\u001b[0m 249ms/step - accuracy: 0.4807 - loss: 0.1251\n",
      "Epoch 3/30\n",
      "\u001b[1m9073/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m2:52\u001b[0m 249ms/step - accuracy: 0.4736 - loss: 0.1129"
     ]
    }
   ],
   "source": [
    "model.fit(Gen,epochs=500,callbacks=[callback],steps_per_epoch=len(SourceImg),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPOdriSsc1qt"
   },
   "outputs": [],
   "source": [
    "model.save('UNET.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KVjUKOthhajV",
    "outputId": "10136de4-b554-4a38-e1db-90b16e7bed8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m6981/9766\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m12:14\u001b[0m 264ms/step - accuracy: 0.4611 - loss: 0.1158"
     ]
    }
   ],
   "source": [
    "callback = ModelCheckpoint('UNET.keras', verbose=1,mode='auto', monitor='loss',save_best_only=True)\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('UNET.keras')\n",
    "model.fit(Gen,epochs=15,callbacks=[callback],steps_per_epoch=len(SourceImg),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KwB8EJu38TZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
